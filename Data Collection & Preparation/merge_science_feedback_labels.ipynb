{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEa1izZiyjZRha+WGboHJj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joongeun/MIT-Internship-2024/blob/main/merge_science_feedback_labels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4JFkSb2mrLh"
      },
      "outputs": [],
      "source": [
        "#Prompt 1 will not be used for Science Feedback because the merged labels are not accurate for credibility scores (e.x.: score of \"0\" means \"Neutral\" but with prompt 1, it would be a negative label)\n",
        "import pandas as pd\n",
        "import re\n",
        "from pprint import pprint\n",
        "import csv\n",
        "\n",
        "df = pd.read_csv('science_feedback_data.csv', index_col=False)\n",
        "\n",
        "#Merging verdicts and credibility scores\n",
        "for ind, i in enumerate(df['verdict']):\n",
        "    p = 0\n",
        "    try:\n",
        "        i = float(i)\n",
        "        if i >= -2 and i < -1:\n",
        "            p = 1\n",
        "        elif i >= -1 and i < 0:\n",
        "            p = 2\n",
        "        elif i == 0:\n",
        "            p = 3\n",
        "        elif i > 0 and i <= 1:\n",
        "            p = 4\n",
        "        elif i > 1 and i <=2:\n",
        "            p = 5\n",
        "    except:\n",
        "        i = i.lower()\n",
        "    if i in [\"accurate\", \"correct\"] or p == 5:\n",
        "        df.loc[ind, 'verdict'] = \"5\"\n",
        "    elif i in [\"mostly correct\", \"mostly accurate\", \"correct but\"] or p == 4:\n",
        "        df.loc[ind, 'verdict'] = \"4\"\n",
        "    elif i in [\"lacks context\", \"imprecise\", \"partially correct\"] or p == 3:\n",
        "        df.loc[ind, 'verdict'] = \"3\"\n",
        "    elif i in [\"unsupported\", \"misleading\"] or p == 2:\n",
        "        df.loc[ind, 'verdict'] = \"2\"\n",
        "    elif i in [\"inaccurate\", \"incorrect\", \"flawed reasoning\"] or p == 1:\n",
        "        df.loc[ind, 'verdict'] = \"1\"\n",
        "\n",
        "#Merging claims and headlines to input to the model\n",
        "inp = []\n",
        "for ind, i in enumerate(df[\"claim\"]):\n",
        "    if type(i) == str:\n",
        "        inp.append(i)\n",
        "    else:\n",
        "        inp.append(df[\"headline\"][ind])\n",
        "\n",
        "df[\"input_claims\"] = inp\n",
        "df.to_csv(\"science_feedback_merged_prompt2.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Only use the input claims/headlines and verdicts to finetune model\n",
        "df = pd.read_csv('science_feedback_merged_prompt2.csv', index_col=False)\n",
        "df = df[df.columns[[10, 1]]]\n",
        "df = df.rename(columns={\"verdict\": \"labels\"})\n",
        "df.to_csv(\"science_feedback_finetune_data.csv\", index=False)"
      ],
      "metadata": {
        "id": "y08exPcWmt6p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}