{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joongeun/MIT-Internship-2024/blob/main/climategpt_finetuning_LoRa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95d0d2f3-7bc2-4523-a64d-c06dc18a55db",
      "metadata": {
        "id": "95d0d2f3-7bc2-4523-a64d-c06dc18a55db"
      },
      "outputs": [],
      "source": [
        "#Install packages and log in to huggingface to upload finetuned model\n",
        "!pip install accelerate peft bitsandbytes transformers trl wandb datasets torch\n",
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70428a57",
      "metadata": {
        "id": "70428a57"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aee4c7e7",
      "metadata": {
        "id": "aee4c7e7",
        "outputId": "420f2d0e-aded-46b5-de4a-ff9d636df0a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['text', 'labels'],\n",
              "     num_rows: 205\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['text', 'labels'],\n",
              "     num_rows: 89\n",
              " }))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Load pre-split datasets\n",
        "base_model = \"eci-io/climategpt-7b\"\n",
        "train_ds = load_dataset(\"csv\", data_files=\"sf_train.csv\", split=\"train\")\n",
        "test_ds = load_dataset(\"csv\", data_files=\"sf_test.csv\", split=\"train\")\n",
        "train_ds, test_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "843373dc",
      "metadata": {
        "id": "843373dc"
      },
      "outputs": [],
      "source": [
        "compute_dtype = getattr(torch, \"float16\")\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5606befc",
      "metadata": {
        "id": "5606befc"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=quant_config,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e32e5ad4",
      "metadata": {
        "id": "e32e5ad4"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea6d7df6",
      "metadata": {
        "id": "ea6d7df6"
      },
      "outputs": [],
      "source": [
        "peft_params = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "training_params = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=25,\n",
        "    logging_steps=25,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    hub_model_id=\"finetuned-climategpt-7b\", #Name of finetuned model when saving\n",
        "    hub_token=\"INSERT_YOUR_HUGGINGFACE_TOKEN_HERE\"\n",
        ")\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_ds,\n",
        "    peft_config=peft_params,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=None,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_params,\n",
        "    packing=False,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf7c1e28",
      "metadata": {
        "id": "cf7c1e28",
        "outputId": "d3dcd949-f209-4bed-ce22-34a2975b5af3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('finetuned_climategpt/tokenizer_config.json',\n",
              " 'finetuned_climategpt/special_tokens_map.json',\n",
              " 'finetuned_climategpt/tokenizer.json')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_model=\"finetuned_climategpt\"\n",
        "# trainer.model.save_pretrained(new_model)\n",
        "# trainer.tokenizer.save_pretrained(new_model)\n",
        "#Uploading model to huggingface hub\n",
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2de9c42",
      "metadata": {
        "id": "d2de9c42",
        "outputId": "61006962-ae3b-4d8a-cf7a-ab49215881dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0: 1\n",
            "1: 5\n",
            "2: 5\n",
            "3: 1\n",
            "4: 1\n",
            "5: 1\n",
            "6: 1\n",
            "7: 1\n",
            "8: 1\n",
            "9: 1\n",
            "10: 5\n",
            "11: 1\n",
            "12: 1\n",
            "13: 4\n",
            "14: 1\n",
            "15: 1\n",
            "16: 2\n",
            "17: 1\n",
            "18: 1\n",
            "19: 1\n",
            "20: 1\n",
            "21: 2\n",
            "22: 2\n",
            "23: 1\n",
            "24: 1\n",
            "25: 4\n",
            "26: 1\n",
            "27: 5\n",
            "28: 5\n",
            "29: 1\n",
            "30: 2\n",
            "31: 1\n",
            "32: 5\n",
            "33: 1\n",
            "34: 5\n",
            "35: 5\n",
            "36: 1\n",
            "37: 5\n",
            "38: 1\n",
            "39: 5\n",
            "40: 4\n",
            "41: 2\n",
            "42: 1\n",
            "43: 1\n",
            "44: 5\n",
            "45: 1\n",
            "46: 5\n",
            "47: 5\n",
            "48: 1\n",
            "49: 2\n",
            "50: 5\n",
            "51: 3\n",
            "52: 1\n",
            "53: 4\n",
            "54: 1\n",
            "55: 4\n",
            "56: 1\n",
            "57: 1\n",
            "58: 1\n",
            "59: 1\n",
            "60: 1\n",
            "61: 1\n",
            "62: 1\n",
            "63: 2\n",
            "64: 5\n",
            "65: 2\n",
            "66: 1\n",
            "67: 2\n",
            "68: 4\n",
            "69: 1\n",
            "70: 5\n",
            "71: 1\n",
            "72: 1\n",
            "73: 1\n",
            "74: 2\n",
            "75: 1\n",
            "76: 4\n",
            "77: 1\n",
            "78: 1\n",
            "79: 4\n",
            "80: 1\n",
            "81: 1\n",
            "82: 1\n",
            "83: 5\n",
            "84: 1\n",
            "85: 5\n",
            "86: 1\n",
            "87: 1\n",
            "88: 2\n",
            "{'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1 for each class': array([1., 1., 1., 1., 1.]), 'f1 weighted': 1.0}\n"
          ]
        }
      ],
      "source": [
        "#Evaluating fine-tuned model on test dataset\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "test_texts = test_ds[\"text\"]\n",
        "test_labels = test_ds[\"labels\"]\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "rows = []\n",
        "for ind, prompt in enumerate(test_texts):\n",
        "    result = pipe(prompt, num_return_sequences=1, max_new_tokens=1)[0]['generated_text'].split(\"ANSWER: \")[1][0]\n",
        "    print(str(ind)+\":\", str(result))\n",
        "    rows.append(result)\n",
        "\n",
        "df = pd.DataFrame({\"outputs\": rows})\n",
        "df.to_csv(\"finetuned_model_outputs.csv\", index=False)\n",
        "\n",
        "def compute_accuracy(path, preds_col):\n",
        "    df = pd.read_csv(path)\n",
        "    y_true = test_labels\n",
        "    y_pred = list(df[preds_col])\n",
        "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
        "    recall = recall_score(y_true=y_true, y_pred=y_pred, average=\"weighted\")\n",
        "    precision = precision_score(y_true=y_true, y_pred=y_pred, average=\"weighted\")\n",
        "    f1 = f1_score(y_true=y_true, y_pred=y_pred, average=None)\n",
        "    f1_weighted = f1_score(y_true=y_true, y_pred=y_pred, average=\"weighted\")\n",
        "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1 for each class\": f1, \"f1 weighted\": f1_weighted}\n",
        "\n",
        "path = \"finetuned_model_outputs.csv\"\n",
        "preds_col = \"outputs\"\n",
        "\n",
        "scores = compute_accuracy(path, preds_col)\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3f94d2b-3cf9-4b7c-9a0b-3121c2544a10",
      "metadata": {
        "id": "c3f94d2b-3cf9-4b7c-9a0b-3121c2544a10"
      },
      "outputs": [],
      "source": [
        "#Importing fine-tuned model from huggingface hub and seeing if it works\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name_or_path = \"Joon007/finetuned-climategpt-7b\" #path/to/your/model/or/name/on/hub\n",
        "device = \"cuda\" # or \"cuda\" if you have a GPU\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "inputs = tokenizer.encode(\"This is a climate-misinformation classification task. Your task is that of telling whether the given text presents a contrarian claim regarding climate change. Your reply should be: 1: incorrect/inaccurate/flawed reasoning, 2: unsupported/misleading, 3: lacks context/imprecise/partially correct, 4: mostly accurate/mostly correct, or 5: accurate/correct. Your reply should contain only the corresponding number and nothing else (i.e., 1, 2, 3, 4, or 5). Terminate your response after including the number. Don't say im_end The claim is: \\\"Most likely the primary control knob [on climate change] is the ocean waters and this environment that we live in.\\\" ANSWER: \", return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs, num_return_sequences=1, max_new_tokens=1)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ab9b830-e5b3-4b64-afd7-926f45e8f596",
      "metadata": {
        "id": "3ab9b830-e5b3-4b64-afd7-926f45e8f596"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}